{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gentle-demographic",
   "metadata": {},
   "source": [
    "# Response and Resolution Studies for the Trigger Level Analysis at the ATLAS Experiment\n",
    "\n",
    "## Abstract\n",
    "When particles collide at the Large Hadron Collider, the invariant mass of new particles can be derived by detecting their decay products. The Trigger Level Analysis at the ATLAS experiment uses partly reconstructed particle collision events for its analysis. The smaller data size allows the Trigger Level Analysis to obtain a much larger number of events and a smaller statistical uncertainty. This notebook presents the data processing and analysis used to plot the invariant mass response. The response is the ratio of invariant mass as determined by particles simulated to interact with the detector, divided by the invariant mass as determined by the same particles, without interaction with the detector. Doing this after each step of the Trigger Level Analysis calibration allows us to see if any calibration step introduces signal-like features in the analysis. The insights gained from the response studies are also used to estimate the invariant mass resolution of the analysis and with that derive an optimal histogram binning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-prospect",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Jets\n",
    "When particles collide at the Large Hadron Collider (LHC) at CERN, it is possible that new particles are created and these particles decay into decay products. The ATLAS detector allows us to study these decay products and learn about the properties of the original particle. Due to the nature of the strong force, one of the most common observable objects at ATLAS are collimated particle showers. From the detector data, we can then group the energy deposits of these particle showers into an object called a jet. By studying these jets we can learn about particles created in the collision, such as the Higgs boson.\n",
    "\n",
    "![alt text](images/jets.png \"Title\")\n",
    "\n",
    "### Invariant Mass and Response\n",
    "Particle collision event data at the LHC uses a specialized data framework and file format called ROOT. We can for each particle collision event access, among other things, the jet's energy and momentum (including direction in the cylindrical coordinates $\\eta$ (eta) and $\\phi$ (phi)). With this, we can represent the jets as [four-vectors](https://en.wikipedia.org/wiki/Four-vector) and calculate the di-jet [invariant mass](https://en.wikipedia.org/wiki/Invariant_mass) ($m_{jj}$) of the particle formed by the collision.\n",
    "\n",
    "Before we run the following notebook, a C++ algorithm has processed millions of simulated particle collision events from Monte Carlo (MC) simulation. With simulated events we can look at the energy and momentum of particles and jets as they would look purely theoretically, we call these jets \"truth jets\". We can also simulate each particle's propagation and interaction with the detector and its final digital readout. This gives us what we call \"reconstructed jets\".\n",
    "\n",
    "Since truth jets only exist in simulation, the simulations allow us to look at the invariant mass ratio between the reconstructed jets and truth jets, which we call response. \n",
    "\n",
    "$$ R = \\frac{m_{jj_{Reco}}}{m_{jj_{Truth}}} $$\n",
    "\n",
    "This response gives us an idea of how much jets measured in reality (reconstructed jets), differ from the truth jets. This in turn gives us important information about the detector, calibration, and data processing.\n",
    "\n",
    "### Jet Calibration\n",
    "Apart from calibration of the individual detector components at ATLAS, we use an MC-driven calibration where we derive different calibration constants and curves by comparing reconstructed objects to truth objects. After the full calibration we want the difference betwee recotnructed an truth jets to be 0, i.e the response to be 1. The figure below shows the Trigger Level Analysis (TLA) calibration steps. Where the final step corrects for disagreement between the MC simulations and actual data.\n",
    "\n",
    "![alt text](images/calibration.png \"Title\")\n",
    "\n",
    "Each calibration step above brings our jets to a different \"energy scale\": EM, Pileup, ETaJES, GSC, in-situ. In this notebook, we will look at the $m_{jj}$ response as a function of the true $m_{jj}$ for the first four energy scales in different slices of the spatial dimension $\\eta$. By looking at the response at each energy scale we can see if any step of the calibration introduces any bumps or features, which would interfere with our final analysis looking for new particles.\n",
    "\n",
    "### Invariant Mass Resolution and Binning\n",
    "Since the $m_{jj}$ distribution is the main observable for TLA in the search for new physics, it is important to use a suitable histogram binning. There are two conflicting considerations in deriving this binning.\n",
    "1. Bins should be wider than the detector resolution to limit the effect from event migrations between bins.\n",
    "2. A narrow binning allows a possible signal excess to spread over multiple bins, allowing for multiple inputs to a fit.\n",
    "\n",
    "The binning is derived by an estimated detector resolution, which we define as the standard deviation of the fitted $m_{jj}$ response distribution at a given truth $m_{jj}$.\n",
    "\n",
    "### Data Processing in this Notebook\n",
    "A root file, with a unique root file path, is distinguished by being from a different campaign (mc16a or mc16d), and being made from different objects (Online jets or Offline jets). For example, in directory v14 we have one root file from the mc16d campaign with online jets. Directory v19 is mc16a with offline jets.\n",
    "\n",
    "The C++ algorithm, which was run before this notebook, creates 3D histograms with $m_{jj}$ response, truth $m_{jj}$, and truth $\\eta$ on the x, y, and z-axis respectively. Each root file contains one 3D histgoram per energy scale in the calibration. This notebook transforms these 3D histograms into more understandable plots such as $m_{jj}$ response vs truth $m_{jj}$ using the following steps:\n",
    "\n",
    "#### Fitting\n",
    "* 3D histograms (called \"TH3\" or \"h3D\" in the code) are sliced on the z-axis and projected into a 2D histogram (each slice is typically in truth $\\eta$).\n",
    "![alt text](images/3d_to_2d.png \"Title\")\n",
    "* 2D histograms (called \"h2D\" in the code) are sliced in bins on the y axis and projected into 1D histograms (each slice is typically a truth $m_{jj}$ bin).\n",
    "![alt text](images/2d_to_1d.png \"Title\")\n",
    "* 1D histograms (called \"h1D\" in the code) are fitted with a gaussian using an external python script called \"JESBalanceFitter\".\n",
    "* Information about the gaussian fits are written into a list of the data series in the dataframe for that $\\eta$ slice.\n",
    "\n",
    "For example, this means that for the mc16d Online ROOT file, we create one pandas data frame per $\\eta$ slice. In the $\\eta$=[-2.8,2.8] slice data frame we store information about the fitted 1D histograms for each 3D histogram as it's own data series, with the 3D histogram name as index.\n",
    "\n",
    "#### Plotting\n",
    "* For each $\\eta$ slice a data frame is read into memory.\n",
    "* For each 3D Histogram in that dataframe, a set of the available fit information is read into memory.\n",
    "* For each 3D histogram, the fit mean is plotted at the corresponding 2D histograms $m_{jj}$ truth bin center. This gives an $m_{jj}$ response vs truth $m_{jj}$ plot for all 3D histograms, for all $\\eta$ slices.\n",
    "![alt text](images/1d_to_graph.png \"Title\")\n",
    "\n",
    "### Final Notes\n",
    "Sometimes we would like to look at a quantity called \"transverse momentum\" (pT) alongside $m_{jj}$, it would be treated the same as $m_{jj}$ in the data processing. For simplicity we only look at $m_{jj}$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-collectible",
   "metadata": {},
   "source": [
    "## Response\n",
    "### Fitting\n",
    "We start by importing the necessary packages. When running this cell, ROOT will ouput a lot of runtime warnings. As long as the last output says `Welcome to JupyROOT 6.20/00` we are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-fiber",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from JES_BalanceFitter import JES_BalanceFitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-contributor",
   "metadata": {},
   "source": [
    "Set the base path to be independent of the directory path and define the paths to the different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList=!pwd\n",
    "path=pathList[0]\n",
    "listOfRootFilePaths = [path+\"/data/v14/merged_mc16d_mjj_v14.root\",\n",
    "                       path+\"/data/v15/merged_mc16d_mjj_v15.root\",\n",
    "                       path+\"/data/v18/merged_mc16a_mjj_v18.root\",\n",
    "                       path+\"/data/v19/merged_mc16a_mjj_v19.root\",\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-fraud",
   "metadata": {},
   "source": [
    "Create a directory to house all the output plots. You can go here to find the plots in PDF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-prediction",
   "metadata": {},
   "source": [
    "Set parameters for the slicing and projecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicingAxis = \"z\"\n",
    "slices = [[-2.8,2.8],[-0.6,0.6]]\n",
    "\n",
    "projectionAxis = \"y\"\n",
    "projectionRebinWidth = 2\n",
    "\n",
    "responseAxis = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-presence",
   "metadata": {},
   "source": [
    "Set parameters for the gaussian fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "nSigmaForFit = 1.3\n",
    "fitOptString = \"RESQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-afternoon",
   "metadata": {},
   "source": [
    "We mainly loop for each file path, for each slice in $\\eta$. Where we first create an empty dataframe for that slice, with one data series for each 3D histogram (one per energy scale). For each 3D histogram we slice, project, and fit the response and save the fit variables to the dataframe data series. With the 3D histogram name as index. This means that for each file path we will get as many response plots as we have $\\eta$ slices, and in each plot we will have as many data series as we have energy scales.\n",
    "\n",
    "The cell below will output a red \"info\" statement when it is done, don't worry about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the file paths\n",
    "for rootFilePath in listOfRootFilePaths:\n",
    "    inFile = ROOT.TFile.Open(rootFilePath) # Open root file containing 3D histograms\n",
    "    \n",
    "    # Loop over the slices\n",
    "    for currentSlice in slices:\n",
    "        listOfKeys = inFile.GetListOfKeys() # Get the 3D histogram names from the root file\n",
    "\n",
    "        dictionaryList=[] # Initialize an empty list to house the dictionaries, one per slice\n",
    "        \n",
    "        # Loop over the 3D Histogram names\n",
    "        for key in listOfKeys:\n",
    "            TH3Name = key.GetName() # Get the actual name as a string\n",
    "            \n",
    "            if TH3Name[0:9] != \"scaled_h_\": # Skip the #D histograms which are not scaled \n",
    "                continue\n",
    "            elif inFile.Get(TH3Name).GetEntries()==0.0: # Print a warning if a 3D histogram is empty\n",
    "                print(\"WARNING:\",TH3Name,\" is empty!\")\n",
    "                continue\n",
    "            else: # If everything is good, append an empty pandas series to the dictionary list\n",
    "                dictionaryList.append(pd.Series({\n",
    "                                     \"x\"             :[],\n",
    "                                     \"y\"             :[],\n",
    "                                     \"xError\"        :[],\n",
    "                                     \"yError\"        :[],\n",
    "                                     \"sigma\"         :[],\n",
    "                                     \"sigmaError\"    :[],\n",
    "                                     \"sigmaOverY\"    :[],\n",
    "                                     \"sigmaOverYError\"    :[],\n",
    "                                     \"fitAmplitude\"  :[],\n",
    "                                     \"fitMin\"        :[],\n",
    "                                     \"fitMax\"        :[],\n",
    "\n",
    "                                     \"TH1BinEdges\"   :[],\n",
    "                                     \"TH1BinEntries\" :[],\n",
    "                                     \"TH1BinErrors\"  :[],\n",
    "                                   },\n",
    "                                      name=TH3Name))\n",
    "        # Define a path and name for the dataframe based on the slice range\n",
    "        dfPath = rootFilePath.split(\".\")[0]+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\n",
    "        print(dfPath)\n",
    "        df = pd.DataFrame(dictionaryList) # Create the dataframe form the dictionary list\n",
    "\n",
    "        # Within the current slice, loop over the 3D histogram names in the empty dataframe\n",
    "        for TH3Name in df.index:\n",
    "            inTH3 = inFile.Get(TH3Name) # Retreive the current 3D histogram form the root file\n",
    "            h3D = inTH3.Clone() # Clone it ot keep it intact\n",
    "\n",
    "            #Set fitting options for the JES_BalanceFitter tool\n",
    "            JESBfitter = JES_BalanceFitter(nSigmaForFit)\n",
    "            JESBfitter.SetGaus()\n",
    "            JESBfitter.SetFitOpt(fitOptString)\n",
    "\n",
    "            # Set the 3D histogram range to correspond to the desired slice range\n",
    "            if slicingAxis == \"y\":\n",
    "                h3D.GetYaxis().SetRangeUser(currentSlice[0], currentSlice[1])     \n",
    "            elif slicingAxis == \"z\":\n",
    "                h3D.GetZaxis().SetRangeUser(currentSlice[0], currentSlice[1]) \n",
    "\n",
    "            # Project the 3D histogram with the sliced axis range into a 2D histogram\n",
    "            h2D=h3D.Project3D(responseAxis + projectionAxis)\n",
    "\n",
    "            # Rebin the 2D histogram x-axis according to the desired rebinningFactor\n",
    "            h2D.RebinX(projectionRebinWidth)\n",
    "\n",
    "            # Loop over the bins in the 2D histogram x-axis\n",
    "            for currentRebinnedBin in range(1, h2D.GetNbinsX()+1): # Histograms start at bin 1, plus one to include last bin\n",
    "                # Name of the 1D projection\n",
    "                projName = \"slice\"+str(currentSlice[0])+\"to\"+str(currentSlice[1])+\"_projectionBin\"+str(h2D.GetXaxis().GetBinLowEdge(currentRebinnedBin))+\"to\"+str(h2D.GetXaxis().GetBinUpEdge(currentRebinnedBin))\n",
    "                \n",
    "                # Project the current 2D histogram bin into a 1D histogram\n",
    "                h1D=h2D.ProjectionY(projName, currentRebinnedBin, currentRebinnedBin)\n",
    "\n",
    "                # If the current 2D bin is empty skip it\n",
    "                if h1D.GetEntries() == 0:\n",
    "                    #print(\"empty 1D hist, skipping!\")\n",
    "                    continue\n",
    "\n",
    "                # Set the fit limits based on the 1D histogram properties\n",
    "                fitMax = h1D.GetMean() + nSigmaForFit * h1D.GetRMS()\n",
    "                fitMin = h1D.GetMean() - nSigmaForFit * h1D.GetRMS()\n",
    "\n",
    "                # Obtain fit using JES_BalanceFitter           \n",
    "                JESBfitter.Fit(h1D, fitMin, fitMax)\n",
    "                fit = JESBfitter.GetFit()\n",
    "                histFit = JESBfitter.GetHisto()\n",
    "                Chi2Ndof = JESBfitter.GetChi2Ndof()\n",
    "\n",
    "                # Initialize empty lists to hold the values of each 1D histogram.\n",
    "                # There will be as many 1D histograms as Y axis bins of the 3D histogram\n",
    "                binEdges=[]\n",
    "                binEntries=[]\n",
    "                binErrors=[]\n",
    "                \n",
    "                # Loop over the response bins in the 1D histogram\n",
    "                for i in range(1, h1D.GetNbinsX()+1):# Plus one to include last bin\n",
    "                  binEdges.append(h1D.GetXaxis().GetBinLowEdge(i)) # Get the current bin edge for plotting later\n",
    "                  binEntries.append(h1D.GetBinContent(i)) # Get the number of entries in the bin for plotting later\n",
    "                  binErrors.append(h1D.GetBinError(i)) # Get the bin error to get the width of the bin... For plotting later\n",
    "                binEdges.append(h1D.GetXaxis().GetBinUpEdge(h1D.GetNbinsX()))# Append the right most edge of the last bin\n",
    "\n",
    "                # Append all the lists for this 1D histogram to the list of lists in the dataframe.\n",
    "                # One dataframe per slice, per root file path\n",
    "                \n",
    "                # Append all the values of the 1D histogram gaussian fit to the lists of the current data frame series\n",
    "                # There are ~4 series per data frame, 1 dataframe per slice, per root file path\n",
    "                df[\"x\"].loc[TH3Name].append(float(h2D.GetXaxis().GetBinCenter(currentRebinnedBin)))\n",
    "                df[\"y\"].loc[TH3Name].append(float(fit.GetParameter(1)))\n",
    "                df[\"xError\"].loc[TH3Name].append(float((h2D.GetXaxis().GetBinWidth(currentRebinnedBin)/2.0)))#half bin width\n",
    "                df[\"yError\"].loc[TH3Name].append(float(fit.GetParError(1)))\n",
    "                df[\"sigma\"].loc[TH3Name].append(float(fit.GetParameter(2)))\n",
    "                df[\"sigmaError\"].loc[TH3Name].append(float(fit.GetParError(2)))\n",
    "                try:\n",
    "                    df[\"sigmaOverY\"].loc[TH3Name].append(float(fit.GetParameter(2) / float(fit.GetParameter(1))))\n",
    "                    df[\"sigmaOverYError\"].loc[TH3Name].append(np.sqrt((fit.GetParError(2)/fit.GetParameter(2))**2+(fit.GetParError(1)/fit.GetParameter(1))**2))\n",
    "                except:\n",
    "                    df[\"sigmaOverY\"].loc[TH3Name].append(0)\n",
    "                    df[\"sigmaOverYError\"].loc[TH3Name].append(0)\n",
    "                \n",
    "                # Append the lists of 1D histogram info to the list of lists in the dataframe data series\n",
    "                df[\"TH1BinEdges\"].loc[TH3Name].append(binEdges)\n",
    "                df[\"TH1BinEntries\"].loc[TH3Name].append(binEntries)\n",
    "                df[\"TH1BinErrors\"].loc[TH3Name].append(binErrors)\n",
    "        \n",
    "        df.to_pickle(dfPath+\".pickle\") # Save the dataframe to disk as pickle. One dataframe per slice per root file path\n",
    "\n",
    "    inFile.Close() # Close the root file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-maldives",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "#### Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-flexibility",
   "metadata": {},
   "source": [
    "Make some helper functions which return whether a data series is from data or Monte Carlo (MC), and whether Online or Offline objects were used. These are mostly used for the plot legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return wether the root file comes from Monte Carlo simulation (MC) or data\n",
    "def isItDataOrMC(rootFilePath):\n",
    "    if(\"mc\" in rootFilePath.split(\"/\")[-1]):\n",
    "        return \"MC\"\n",
    "    elif(\"data\" in rootFilePath.split(\"/\")[-1]):\n",
    "        return \"Data\"\n",
    "    else:\n",
    "        \"Cannot determine if it is Data or MC\"\n",
    "    \n",
    "# Return wether the 3D histogram is from Online or Offline objects\n",
    "def isItOnlineOrOffline(dfIndex):\n",
    "    if(\"Online\" in dfIndex):\n",
    "        return \"Online\"\n",
    "    elif(\"Offline\" in dfIndex):\n",
    "        return \"Offline\"\n",
    "    else:\n",
    "        return \"Cannot determine if it is Online or Offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-month",
   "metadata": {},
   "source": [
    "Set plotting parameters such as axis ranges, axis labels, legend titles, and energy scales to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "xAxisUnit = \"mjj\" # Usually mjj, pT, or energy. Essentaily the variable name of the response\n",
    "\n",
    "xAxisLabel = \"Truth $\"+xAxisUnit[0]+\"_{\"+xAxisUnit[1:].upper()+\"}$ [GeV]\"\n",
    "yAxisLabel = \"$\"+xAxisUnit[0]+\"_{\"+xAxisUnit[1:].upper()+\"}$ Response\"\n",
    "\n",
    "skipEnergyScales = [\"SmearedMomentum\"] # A list of energy scale names to skip\n",
    "\n",
    "# The legend titles are lists of strings, where parameters from the code are input\n",
    "# between and after the different title entries\n",
    "responseLegendTitle = [\"Energy Scale Response\",\n",
    "               \"$\\eta = $\",\n",
    "              ]\n",
    "\n",
    "resolutionLegendTitle = [\"Online vs. Offline Resolution\",\n",
    "               \"$\\eta = $\",\n",
    "              ]\n",
    "resolutionRatioLegendTitle = [\"Resolution Ratio:\",\n",
    "              ]\n",
    "\n",
    "# Set the x and y-axis limits\n",
    "xLimits = (100,5000)\n",
    "yLimits = (0.5,1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-custom",
   "metadata": {},
   "source": [
    "Import packages needed for plotting and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as scipy\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-bedroom",
   "metadata": {},
   "source": [
    "For each filepath and slice, we plot the response for the desired xAxisUnit (for example $m_{jj}$), skipping the energy scales we want to skip.\n",
    "\n",
    "The cell below will output warnings about type 3 fonts. Don't worry about it, the plots will appear at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the file paths\n",
    "for rootFilePath in listOfRootFilePaths:\n",
    "    # Loop over the slices\n",
    "    for currentSlice in slices:\n",
    "        # Initialize figure 185 mm wide, wiht a 800:600 widht:height ratio\n",
    "        f, ax = plt.subplots(figsize=(18.3*(1/2.54), 13.875*(1/2.54)))\n",
    "        \n",
    "        # Define a path and name for the dataframe based on the slice range\n",
    "        dfPath = rootFilePath.split(\".\")[0]+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\n",
    "        df = pd.read_pickle(dfPath+\".pickle\") # Read the appropriate dataframe. One per slice, per file path\n",
    "        \n",
    "        # Get data information based on root file path and TH3 name\n",
    "        DataOrMC = isItDataOrMC(rootFilePath)\n",
    "        OnlineOrOffline = isItOnlineOrOffline(df.index[0])\n",
    "        campagin = rootFilePath.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
    "        \n",
    "        # Define markers and colors for plot data series\n",
    "        markers = [\"o\",\"^\",\">\",\"v\",\"<\"]\n",
    "        colors = [\"black\",\"crimson\",\"darkorange\",\"dodgerblue\",\"forestgreen\"]\n",
    "        i=0 # Set an iterator for the markes and colors\n",
    "        \n",
    "        # Iterate over TH3s in data frame, in the reverse order\n",
    "        for TH3Name in reversed(df.index):\n",
    "            skipping = False # Set upp skipping flag\n",
    "            if xAxisUnit not in TH3Name: # Skip pT plots if we are interested in mjj. For example\n",
    "                skipping = True\n",
    "            # A quick loop over the list of skipped energy scales\n",
    "            for skippedEnergyScales in skipEnergyScales: # Skip if the 3D histogram name is in the list to skip\n",
    "                if skippedEnergyScales in TH3Name:\n",
    "                    skipping = True\n",
    "            if skipping: continue\n",
    "                \n",
    "            # Get data information based on TH3 name\n",
    "            numeratorEnergyScale = TH3Name.split(\"_-_\")[1].split(\"_\")[0].split(\"-\")[0]\n",
    "            denominatorEnergyScale = TH3Name.split(\"_-_\")[1].split(\"_\")[2].split(\"-\")[1]\n",
    "\n",
    "            # Assign data from dataframe for the current 3D histogram name\n",
    "            x=df[\"x\"].loc[TH3Name]\n",
    "            y=df[\"y\"].loc[TH3Name]\n",
    "            x_error=df[\"xError\"].loc[TH3Name]\n",
    "            y_error=df[\"yError\"].loc[TH3Name]\n",
    "\n",
    "            # Plot data\n",
    "            ax.errorbar(x, y, yerr=y_error, xerr=x_error,\n",
    "                        linestyle='None',\n",
    "                        marker=markers[i],\n",
    "                        color=colors[i],\n",
    "                        markersize=2,\n",
    "                        linewidth=0.5,\n",
    "                        label=numeratorEnergyScale+\"$_{\"+xAxisUnit+\"}$\"+\" / \"+denominatorEnergyScale.capitalize()+\"$_{\"+xAxisUnit+\"}$\",\n",
    "                       )\n",
    "            i+=1 # Increment color and marker iterator\n",
    "\n",
    "        # Add legend\n",
    "        leg = ax.legend(borderpad=0.5, loc=1, ncol=2, frameon=True,facecolor=\"white\",framealpha=1)\n",
    "        leg._legend_box.align = \"left\"\n",
    "        leg.set_title(campagin+\" \"+OnlineOrOffline+\" \"+responseLegendTitle[0]+\"\\n\"+responseLegendTitle[1]+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\")\n",
    "\n",
    "        # Set limits and labels\n",
    "        ax.set_xlim(xLimits)\n",
    "        ax.set_ylim(yLimits)\n",
    "\n",
    "        # Set log scale\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel(xAxisLabel, fontsize=14, ha='right',x=1.0)\n",
    "        ax.set_ylabel(yAxisLabel, fontsize=14, ha='right', y=1.0)\n",
    "\n",
    "\n",
    "        # Add grid and custom tick markers\n",
    "        ax.grid(True)\n",
    "        tickList = [1,2,3,4,5,6,7,8,9,\n",
    "        10,20,30,40,50,60,70,80,90,\n",
    "        100,200,300,400,500,600,700,800,900,\n",
    "        1000,2000,3000,4000,5000,6000,7000,8000,9000,\n",
    "        10000]\n",
    "        ax.set_xticks(tickList[tickList.index(xLimits[0]):tickList.index(xLimits[1])])\n",
    "        ax.set_xticklabels(tickList[tickList.index(xLimits[0]):tickList.index(xLimits[1])])\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Add ATLAS label\n",
    "        hep.atlas.text(\"Internal\",ax=ax)\n",
    "\n",
    "        # Use tight layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot as .pdf\n",
    "        f.savefig(\"output/\"+campagin+\"_\"+OnlineOrOffline+\"_\"+xAxisUnit+\"_\"+\"response\"+\"_\"+\"eta\"+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-preservation",
   "metadata": {},
   "source": [
    "#### Bokeh\n",
    "\n",
    "To be able to see how good the 1D response distributions are for each data point in the above plots, we create an interactive plot using bokeh. In the first half we just create a normal bokeh plot. In the second part we loop over each datapoint in the plot, and create a histogram based on the 1D histogram data saved for each datapoint in the dataframe. For these subplots we dump a JSON item into a list, which is later used for a custom hover tooltip in bokeh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3da4e9",
   "metadata": {},
   "source": [
    "Import packages for bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bokeh.embed import json_item\n",
    "from bokeh.models import ColumnDataSource, CustomJSHover, HoverTool\n",
    "from bokeh.plotting import figure, show, output_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14fd86",
   "metadata": {},
   "source": [
    "Define one specific data series, from one slice, from one file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootFilePath = listOfRootFilePaths[0]\n",
    "TH3Name = \"scaled_h_-_GSC-Online_over_-truth_-_mjj_-_eta\"\n",
    "currentSlice=[-2.8,2.8]\n",
    "dfPath = rootFilePath.split(\".\")[0]+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\n",
    "df= pd.read_pickle(dfPath+\".pickle\")\n",
    "series = df.loc[TH3Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a40f7",
   "metadata": {},
   "source": [
    "Assign the data, create the bokeh data structure \"source\", the figure, and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data from dataframe for the current 3D histogram name\n",
    "x = series['x']\n",
    "y = series['y']\n",
    "\n",
    "source = ColumnDataSource(data={'x':x,'y':y})\n",
    "#source = ColumnDataSource(data=dict(x=[1, 4, 8], y=[2, 8, 5]))\n",
    "\n",
    "p = figure(width=800, height=600,\n",
    "           x_axis_label='mjj [GeV]', y_axis_label='mjj Response',\n",
    "           x_range=(100,5000),y_range=(0.97,1.01),\n",
    "           x_axis_type=\"log\",\n",
    "           tools=\"pan,box_zoom,wheel_zoom,reset\",\n",
    "           tooltips=[('mjj','@x'),('Response','@y')]\n",
    "          )\n",
    "\n",
    "p.circle('x','y',source=source,legend_label=TH3Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa73374",
   "metadata": {},
   "source": [
    "For each data point in the plot, create a histogram from the 1D histogram data stored in the dataframe. Then dump this histogram into a json and add it into a new bokeh source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9738d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfOut = [] # initialize an empty list ot hold histograms for each data point\n",
    "\n",
    "# Loop over the 1D histograms / data points in the data series we are looking at\n",
    "for i,TH1 in enumerate(x):\n",
    "    # Initialize a bokeh figure\n",
    "    p2 = figure(title=\"mjj = \"+str(TH1)+\"GeV\",height=400, width=600, toolbar_location=None,\n",
    "                x_axis_label='mjj Response [GeV]', y_axis_label='Entries',\n",
    "                x_range=(0.5,1.5)\n",
    "               )\n",
    "    # Add a histogram based on the 1D histogram we fitted in the dataframe to the second bokeh plot\n",
    "    p2.quad(top=series[\"TH1BinEntries\"][i], bottom=0, left=series[\"TH1BinEdges\"][i][:-1], right=series[\"TH1BinEdges\"][i][1:],\n",
    "            fill_color=\"navy\",\n",
    "            line_width=0,\n",
    "           )\n",
    "    # Dump the plot as a json item into the list\n",
    "    listOfOut.append(json.dumps(json_item(p2, \"tooltip-plot\")))\n",
    "\n",
    "# Create a bokeh data surce from the list\n",
    "source.data['plot'] = listOfOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640715d",
   "metadata": {},
   "source": [
    "Define some Java code and add a custom hover tooltip to the first bokeh plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some Java code\n",
    "code=\"\"\"\n",
    "    Bokeh.embed.embed_item(JSON.parse(value))\n",
    "    return \"\"\n",
    "\"\"\"\n",
    "\n",
    "# Add a custom hover tooltip to the first bokeh plot\n",
    "p.add_tools(HoverTool(\n",
    "    tooltips='<div id=\"tooltip-plot\">@plot{custom}</div>',\n",
    "    formatters={ '@plot': CustomJSHover(code=code) }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2efe0",
   "metadata": {},
   "source": [
    "Show the bokeh plot. Turn off the top hover tool for a better experience. The toolbar on the right should look like this:\n",
    "![alt text](images/like_this.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e76c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook() # Remove this if running locally to get more space for the individual 1D histograms\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-synthetic",
   "metadata": {},
   "source": [
    "## Resolution\n",
    "For the resolution we want to compare the resolution of the TLA online $m_{jj}$, to the ATLAS main physics analysis' offline $m_{jj}$. Therefore we select only two of the root file paths defined in the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfComparisonPaths = [\n",
    "                listOfRootFilePaths[0],\n",
    "                listOfRootFilePaths[1],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-video",
   "metadata": {},
   "source": [
    "We then select the final enegy scale and response variable, since comparing all of them would be too many, and in the end, it is the final energy scale that really matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "energyScale = \"GSC\"\n",
    "xAxisUnit = \"mjj\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-lindsay",
   "metadata": {},
   "source": [
    "Next we define a function taking the form of the energy resolution of a calorimeter detector. We will use this to fit our data, since ATLAS uses a calorimeter to measure energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolutionFunc(x,a,b,c):\n",
    "    #return np.sqrt((a/x)+(b/x)**2+c)\n",
    "    return np.sqrt((a/np.sqrt(x))**2+(b/x)**2+c**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-brunei",
   "metadata": {},
   "source": [
    "Lastly we loop over the $\\eta$ slices. For each slice we loop over the root file paths which we want to compare, one with online histograms and one with offline histograms. For each file we select the 3D histogram name which corresponds to our energyscale and response variable defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-appreciation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linspace=np.linspace(x[0],x[-1],num=10000) # Define a linspace to be used for plotting the fit\n",
    "color_list=[\"red\",\"black\"] # Define the colors used by the two data series\n",
    "\n",
    "#guesses = [[0.937760293466364, 11.803208703991855, 0.13229820524075314],[0.9208120809096343, -9.716149218976808, 0.13347830553680345]]\n",
    "values = [[]]\n",
    "# Loop over the slices\n",
    "for currentSlice in slices:\n",
    "    # Initialize figure 185 mm wide, wiht a 800:600 widht:height ratio\n",
    "    f, axisList = plt.subplots(2,1,figsize=(18.3*(1/2.54), 13.875*(1/2.54)),sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Initialize empty lists of lists to hold the two data series.\n",
    "    # This is later used for a ratio plot\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # Loop over the two file paths\n",
    "    for i,rootFilePath in enumerate(listOfComparisonPaths):\n",
    "        # Define a path and name for the dataframe based on the slice range\n",
    "        dfPath = rootFilePath.split(\".\")[0]+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\n",
    "        df= pd.read_pickle(dfPath+\".pickle\") # Read the appropriate dataframe. One per slice, per file path\n",
    "        \n",
    "        # Get data information based on root file path and TH3 name\n",
    "        DataOrMC = isItDataOrMC(rootFilePath)\n",
    "        OnlineOrOffline = isItOnlineOrOffline(df.index[0])\n",
    "\n",
    "        # Only select the data series / 3D histogram following our earlier specifications\n",
    "        TH3Name = df[[((energyScale in s) and (xAxisUnit in s)) for s in df.index]].index[0]\n",
    "        \n",
    "        # Assign data from data series\n",
    "        x=df[\"x\"].loc[TH3Name]\n",
    "        x_list.append(x) # Append list for the ratio plot\n",
    "        y=df[\"sigmaOverY\"].loc[TH3Name] # The resolution is defined as the error of the response fit mean over the response mean\n",
    "        y_list.append(y) # Append list for the ratio plot\n",
    "        x_error=df[\"xError\"].loc[TH3Name]\n",
    "        y_error=df[\"sigmaOverYError\"].loc[TH3Name]\n",
    "\n",
    "        # Plot the resolution\n",
    "        axisList[0].errorbar(x, y, yerr=y_error, xerr=x_error,\n",
    "                    linestyle='None',\n",
    "                    marker=\"o\",\n",
    "                    color=color_list[i],\n",
    "                    markersize=2,\n",
    "                    linewidth=0.5,\n",
    "                    label=rootFilePath.split(\"/\")[-1].split(\"_\")[1]+\" \"+OnlineOrOffline)\n",
    "\n",
    "        # Fit the resolution with the resolution function\n",
    "        popt_resolutionFunc, pcov_resolutionFunc = curve_fit(resolutionFunc, x, y, sigma=y_error)\n",
    "\n",
    "        print(currentSlice,rootFilePath.split(\"/\")[-1].split(\"_\")[1]+\" \"+OnlineOrOffline)\n",
    "        #print(popt_resolutionFunc)\n",
    "        print(f\"[{np.sqrt(popt_resolutionFunc[0])},{popt_resolutionFunc[1]},{np.sqrt(popt_resolutionFunc[2])}]\")\n",
    "        \n",
    "        # Plot the fit\n",
    "        axisList[0].plot(linspace, resolutionFunc(linspace,*popt_resolutionFunc), lw=1, label=rootFilePath.split(\"/\")[-1].split(\"_\")[1]+OnlineOrOffline+\" Fit\",color=color_list[i])\n",
    "\n",
    "    # Add the fit from 2016 Online resolution\n",
    "    #axisList[0].plot(linspace[240:], resolutionFunc(linspace[240:],0.27,10.6,0.039), lw=1, label=r'2016 Online Fit',color=\"blue\")\n",
    "    #axisList[0].plot(linspace[:240], resolutionFunc(linspace[:240],0.27,10.6,0.039), lw=1, label=r'2016 Extrapolated Fit',color=\"blue\",linestyle='dashed')\n",
    "    \n",
    "    axisList[0].plot(linspace[240:], resolutionFunc(linspace[240:],0.937760293466364,11.803208703991855,0.13229820524075314), lw=1, label=r'test Online',color=\"grey\")\n",
    "    axisList[0].plot(linspace[240:], resolutionFunc(linspace[240:],0.9208120809096343,-9.716149218976808,0.13347830553680345), lw=1, label=r'test Offline',color=\"grey\")\n",
    "    \n",
    "    # Legend\n",
    "    leg = axisList[0].legend(borderpad=0.5, frameon=True, loc=1,ncol=2,facecolor=\"white\",framealpha=1.0)\n",
    "    leg.set_title(resolutionLegendTitle[0]+\"\\n\"+resolutionLegendTitle[1]+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\")\n",
    "    leg._legend_box.align = \"left\"\n",
    "\n",
    "    axisList[0].set_ylabel(r'$m_{jj}$ Resolution', fontsize=14, ha='right', y=1.0)\n",
    "    \n",
    "    hep.atlas.text(\"Simulation Internal\",ax=ax)\n",
    "\n",
    "    # Set limits and labels of the top plot\n",
    "    axisList[0].set_xlim(100,5000)\n",
    "    axisList[0].set_xscale(\"log\")\n",
    "    axisList[0].set_ylim(0.00,0.2)\n",
    "    axisList[0].grid()\n",
    "\n",
    "    # Plot the ratio of Online resolution over Offline resolution\n",
    "    axisList[1].errorbar(np.array(x_list[0]),np.array(y_list[0])/np.array(y_list[1]),linestyle='None',marker=\"o\",color=\"black\",markersize=2,linewidth=0.5,label=\"Ratio\")\n",
    "\n",
    "    # Set limits and grid to the ratio plot\n",
    "    axisList[1].set_ylim(0.98,1.2)\n",
    "    axisList[1].grid()\n",
    "    \n",
    "    # Add 10% graphic lines\n",
    "    axisList[1].plot( [100,5000], [1.0,1.0],color=\"red\",lw=1)\n",
    "    axisList[1].plot( [100,5000], [1.1,1.1],color=\"red\",lw=1)\n",
    "\n",
    "    # Add custom tick markers\n",
    "    axisList[1].set_axisbelow(True)\n",
    "    axisList[1].set_xticks([100,400,1000,5000])\n",
    "    axisList[1].set_xticklabels([100,400,1000,5000])\n",
    "\n",
    "    # Add axis labels\n",
    "    axisList[1].set_xlabel(r\"Truth $m_{jj}$ [GeV]\", fontsize=14, ha='right', x=1.0)\n",
    "    axisList[1].set_ylabel(r\"Online/Offline\", fontsize=14)\n",
    "\n",
    "    # Add ATLAS label\n",
    "    hep.atlas.text(\"Internal\",ax=axisList[0])\n",
    "    \n",
    "    # Use tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot as .pdf\n",
    "    f.savefig(\"output/\"+rootFilePath.split(\"/\")[-1].split(\"_\")[1]+\"_\"+\"onlineVsOffline\"+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"+\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-conducting",
   "metadata": {},
   "source": [
    "## Resolution Based Binning\n",
    "We will use the resolution to estimate the resolution of the detector, and with that obtain the optimal $m_{jj}$ histogram binning. Firstly we define the same resolution function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolutionFunc(x,a,b,c):\n",
    "    return np.sqrt((a/np.sqrt(x))**2+(b/x)**2+c**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-tattoo",
   "metadata": {},
   "source": [
    "Next we pick the data series we want to use for obtaining the new binning. For this we want to use the highest energy scale, GSC, the large $\\eta$ slice -2.8 to 2.8, from the mc16d Online filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootFilePath = listOfRootFilePaths[0]\n",
    "TH3Name = \"scaled_h_-_GSC-Online_over_-truth_-_mjj_-_eta\"\n",
    "currentSlice=[-2.8,2.8]\n",
    "dfPath = rootFilePath.split(\".\")[0]+\"_\"+slicingAxis+\"[\"+str(currentSlice[0])+\",\"+str(currentSlice[1])+\"]\"\n",
    "df= pd.read_pickle(dfPath+\".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-aruba",
   "metadata": {},
   "source": [
    "Next we assign variables to the appropriate lists of the dataframe and fit this data with a resolution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[\"x\"].loc[TH3Name]\n",
    "y=df[\"sigmaOverY\"].loc[TH3Name]\n",
    "x_error=df[\"xError\"].loc[TH3Name]\n",
    "y_error=df[\"sigmaOverYError\"].loc[TH3Name]\n",
    "\n",
    "popt_resolutionFunc, pcov_resolutionFunc = curve_fit(resolutionFunc, x, y, sigma=y_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-water",
   "metadata": {},
   "source": [
    "We want at least one bin edge to be at 531 GeV, since this bin edge coincides with previous studies. We therefore start at 531 and loop down to 100. For each iteration we define a bin width as the rounded integer of the resolution function evaluated at the current bin edge, times the current bin edge. The next iteration starts from the first bin edge minus the calculated bin width, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins1 = [] # Initialize an empty list to hold the bottom half of the list of bin edges\n",
    "currentEdge = 531 # Define the starting bind edge\n",
    "\n",
    "# Loop until the bin edge is less than 100\n",
    "while (currentEdge > 100):\n",
    "    # Calculate a new bin width by evaluating the resolution fit at the current edge\n",
    "    currentBinwidth = int(round( resolutionFunc(currentEdge, *popt_resolutionFunc)*currentEdge ))\n",
    "    currentEdge -= currentBinwidth # Define a new current edge by subtraction\n",
    "    bins1.append(currentEdge) # Append the new current edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-wiring",
   "metadata": {},
   "source": [
    "We then do the same but in reverse from 531 up to 5000. For each evaluated bin width we add that bin width to the current bin edge instead of subtracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-filing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bins2 = [] # Initialize an empty list to hold the top half of the list of bin edges\n",
    "currentEdge = 531# Define the starting bind edge\n",
    "\n",
    "# Loop until the bin edge is less than 100\n",
    "while (currentEdge < 5000):\n",
    "    # Calculate a new bin width by evaluating the resolution fit at the current edge\n",
    "    currentBinwidth = int(round( resolutionFunc(currentEdge, *popt_resolutionFunc)*currentEdge ))\n",
    "    currentEdge += currentBinwidth # Define a new current edge by addition\n",
    "    bins2.append(currentEdge) # Append the new current edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce590ac3",
   "metadata": {},
   "source": [
    "Merge the two list of bin edges, append the starting bin edge, and sort the list in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536af966",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=bins1+bins2\n",
    "bins.append(531)\n",
    "bins.sort()\n",
    "\n",
    "print(\"Bin edges: \\n\",bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-genius",
   "metadata": {},
   "source": [
    "Next we write the binning to a JSON file for easy sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a dictionary of different types of binning, one containing our binning\n",
    "mjjBins = {\n",
    "    'TLAdefault':   bins,\n",
    "    'TLAlowMu'  :   []\n",
    "}\n",
    "\n",
    "outfile = open('output/mjjBins.json','w') # Open a JSON file\n",
    "# Write the dictionary to the JSON file\n",
    "outfile.write(json.dumps(mjjBins, sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98bd96",
   "metadata": {},
   "source": [
    "This JSON file of bin edges will be stored centrally and provide a optimal binning for the whole analysis team to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bce3424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pekman/resolution/responseStudies-alex-exook\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/pekman/resolution/responseStudies-alex-exook'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2c468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfaf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
